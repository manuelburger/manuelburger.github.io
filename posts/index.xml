<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>All Posts - Manuel Burger</title><link>https://manuelburger.ch/posts/</link><description>All Posts | Manuel Burger</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Manuel Burger</copyright><lastBuildDate>Fri, 16 Feb 2024 18:01:40 +0100</lastBuildDate><atom:link href="https://manuelburger.ch/posts/" rel="self" type="application/rss+xml"/><item><title>Best Paper Award at AIM-FM for 'Towards Foundation Models for Critical Care Time Series'</title><link>https://manuelburger.ch/towardsicufm/</link><pubDate>Fri, 16 Feb 2024 18:01:40 +0100</pubDate><author>Manuel Burger</author><guid>https://manuelburger.ch/towardsicufm/</guid><description><![CDATA[<ul>
<li><strong>Paper</strong>: <a href="https://arxiv.org/abs/2411.16346" target="_blank" rel="noopener noreffer ">https://arxiv.org/abs/2411.16346</a></li>
</ul>
<h2 id="abstract">Abstract</h2>
<p>Notable progress has been made in generalist medical large language models across various healthcare areas. However, large-scale modeling of in-hospital time series data - such as vital signs, lab results, and treatments in critical care - remains underexplored. Existing datasets are relatively small, but combining them can enhance patient diversity and improve model robustness. To effectively utilize these combined datasets for large-scale modeling, it is essential to address the distribution shifts caused by varying treatment policies, necessitating the harmonization of treatment variables across the different datasets. This work aims to establish a foundation for training large-scale multi-variate time series models on critical care data and to provide a benchmark for machine learning models in transfer learning across hospitals to study and address distribution shift challenges. We introduce a harmonized dataset for sequence modeling and transfer learning research, representing the first large-scale collection to include core treatment variables. Future plans involve expanding this dataset to support further advancements in transfer learning and the development of scalable, generalizable models for critical healthcare applications.</p>]]></description></item><item><title>Multi-modal Graph Learning over UMLS Knowledge Graphs</title><link>https://manuelburger.ch/mmugl/</link><pubDate>Thu, 09 Nov 2023 18:01:40 +0100</pubDate><author>Manuel Burger</author><guid>https://manuelburger.ch/mmugl/</guid><description><![CDATA[<ul>
<li><strong>Paper</strong>: <a href="https://proceedings.mlr.press/v225/burger23a.html" target="_blank" rel="noopener noreffer ">https://proceedings.mlr.press/v225/burger23a.html</a></li>
<li><strong>GitHub</strong>: <a href="https://github.com/ratschlab/mmugl" target="_blank" rel="noopener noreffer ">https://github.com/ratschlab/mmugl</a></li>
</ul>
<h2 id="abstract">Abstract</h2>
<p>Clinicians are increasingly looking towards machine learning to gain insights about patient evolutions. We propose a novel approach named Multi-Modal UMLS Graph Learning (MMUGL) for learning meaningful representations of medical concepts using graph neural networks over knowledge graphs based on the unified medical language system. These representations are aggregated to represent entire patient visits and then fed into a sequence model to perform predictions at the granularity of multiple hospital visits of a patient. We improve performance by incorporating prior medical knowledge and considering multiple modalities. We compare our method to existing architectures proposed to learn representations at different granularities on the MIMIC-III dataset and show that our approach outperforms these methods. The results demonstrate the significance of multi-modal medical concept representations based on prior medical knowledge.</p>]]></description></item></channel></rss>